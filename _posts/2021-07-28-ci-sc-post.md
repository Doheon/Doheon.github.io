---
title: "[코드구현] Sentence Classification - Doc2Vec, LSTM, KoBERT"
toc_sticky: true
date: 2021-07-28
categories: Code-Implementation NLP
---

뉴스의 본문을 가지고 뉴스의 8가지의 카테고리를 분류하는 동일한 task를 Doc2Vec, 임베딩 후 LSTM, KoBERT 이렇게 세 가지의 방법으로 진행해 보았다.



작성한 코드는 아래와 같다.

Doc2Vec:

FastText + LSTM:

KoBERT: 



각 모델들의 최종 성능은 아래와 같다.

| Method          | Accuracy |
| --------------- | -------- |
| Doc2Vec         | 81.0%    |
| KoBERT          | 86.8%    |
| FastText + LSTM | 81.8%    |



test set에 대한 정확도는 KoBERT>LSTM>=Doc2Vec 으로 나왔다.

하지만 직접 입력한 문장에 대한 결과는 KoBERT가 압도적으로 좋게 느껴졌다. 다른 모델들은 고정된 길이에 대해서 인풋을 받아서 병목현상이 발생하지만 BERT 모델은 attention mask를 통해 다양한 길이에 대해 처리를 할 수 있기 때문이라고 생각된다.

KoBERT를 사용하는 방법이 별도의 토큰화나, 임베딩을 따로 구현하지 않아서 가장 구현하기 쉬웠음에도 불구하고 가장 좋은 성능을 가지고 있었다.

확실히 자연어 처리에는 BERT, GPT와 같은 Transformer기반의 모델의 성능이 매우 뛰어나다는 것을 느꼈다.



사용한 데이터 셋은 아래와 같다.

Dataset: <http://ling.snu.ac.kr/class/cl_under1801/FinalProject.htm>
