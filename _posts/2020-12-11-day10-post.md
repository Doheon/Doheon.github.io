---
title: "[인공지능 데브코스] 2주차 day5 - 추정, 검정, 엔트로피"
toc: true
toc_sticky: true
date: 2020-12-11
categories: TIL 수학
use_math: true
---

## 12월 11일 금   

오늘은 통계학 공부의 마지막날로 추정, 검정, 엔트로피에 대해 공부했다.  
옛날에 대학교 수업 들었을때 열심히 외웠던 것들을 다시 한번 만나게 됐다.  
분량이 많았지만 문제푸는 시험보는 것도 아니고 공식들은 필요할 때 여기서 찾아보면 되니까 각 개념들이 언제 왜 쓰이는지 확실히 이해하고 넘어가면 될 거 같다.  
엔트로피 부분에서 처음으로 머신러닝얘기가 나왔는데 확실히 통계학이 인공지능을 이해하는데 도움이 된다는 것을 알 수 있었다.  
내일 주말이라서 그런지 기분이 좋다. 블로그 올리는걸 안해도 돼서 그런것 같다.  


## 표본분포 (Sampling Distribution) 

통계적 추론
- 표본조사를 통해 모집단에 대한 해석
- 전수조사는 실질적으로 불가능한 경우가 많음

표본조사는 반드시 오차가 발생
- 적절한 표본 추출 방법이 필요
- 표본과 모집단과의 **관계**를 이해해야 함


### 표본분포

표본조사를 통해 파악하고자 하는 정보: **모수 (parameter)**  

모수의 종류: 모평균, 모분산, 모비율 등  
=> 모수를 추정하기 위해 표본을 선택하여 표본 평균이나 표본 분산 등 계산  
<p>&nbsp;</p>  

**통계량 (statistic)**  
표본 평균이나 표본 분산과 같은 표본의 특성값  
=> 이러한 값들도 확률변수라고 할 수 있다.  
(표본 평균이 특정값일 확률을 나타내는 방식)  
<p>&nbsp;</p>  

**표본 평균이 가질 수 있는 값도 하나의 확률분포를 가진다!**  
주로 가장 관심을 많이 가지는 통계량은 모평균이다. 따라서 주로 **표본평균**을 다룬다.   
<p>&nbsp;</p>  

### 표본평균  

모평균을 알아내는데 쓰이는 통계량  

**정규분포에서 표본평균의 분포**  
표본이 **정규분포인 모집단**에서 추출된 값일 때 모집단의 평균과 분산이 아래와 같다면  

평균: $\mu$, 분산: $\sigma^2$  

표본평균역시 **정규분포**를 이루며 표본평균분포의 평균과 분산은 아래와 같다.  

평균: $\mu$, 분산: $\dfrac{\sigma^2}{n}$  

평균은 그대로지만 분산은 n이 커질수록 작아지는 것을 확인 할 수 있다.  
=> 관측하는 데이터가 많을 수록 표본평균의 값이 평균에 모여있게 되고 모집단과 표본집단이 비슷해진다. (당연)  
<p>&nbsp;</p>  

### 중심극한정리 (Central Limit Theorem)  

**임의의** 모집단에서 추출된 표본의 측정값이 아래와 같을 때  
평균: $\mu$, 분산: $\sigma^2$  

표본평균은 n이 충분히 클 때 ($n \ge 30$) 아래와 같은 분포를 따른다.  

$\bar{x} = \dfrac{1}{n} \sum x_i$  
$\bar{X} \sim N(\mu, \dfrac{\sigma^2}{n})$  

**모집단이 정규 분포가 아니어도 n이 커지면 표본평균은 정규분포를 따른다!**  
말도안돼..

정규분포면 n에 상관없이 정규 분포고 정규 분포가 아니더라도 n이 30이상이면 정규분포를 따른다.  
유용하게 쓰이는 성질이다.  

## 추정 (estimation)

### 모평균의 추정

표본평균의 특성
- 모집단이 정규분포인 경우: 표본평균도 정규분포를 따른다.
- 대표본인 경우 (n>30): 중심극한 정리에 의해 표본평균이 정규분포를 따른다고 가정한다.  

=> 그냥 표본평균은 정규분포를 따른다고 가정하고 추정을 시행한다.  

**점추정**  
표본평균이 점 추정값이 된다.  
점 추정만 가지고는 어떤 의미를 가지는지 설명하기는 정보의 양이 적기 때문에 조금 힘들다.  
<p>&nbsp;</p>  

**구간추정**    
신뢰구간(confidence interval) 이 제시되었을 때 해당 신뢰 구간을 만족하는 구간을 추정하는 것  

- 모평균 $\mu$의 $100(1-\alpha)%$ 신뢰구간  

($\mu$의 추정량) $\pm z_{\alpha / 2}$  

- 정규분포에서 $\sigma$를 알 때  

$(\bar{x} - z_{\alpha / 2} \dfrac{\sigma}{\sqrt{n} },\ \bar{x} + z_{\alpha / 2} \dfrac{\sigma}{\sqrt{n} })$  

- 표본의 크기가 큰 경우: 중심 극한 정리를 사용하여 적용시칸다.  

$(\bar{x} - z_{\alpha / 2} \dfrac{s}{\sqrt{n} },\ \bar{x} + z_{\alpha / 2} \dfrac{s}{\sqrt{n} })$  

$s$: 표본표준편차  

$z_{\alpha/2}$ 값은 함수로 구하거나 표준정규분포표에서 확인  
<p>&nbsp;</p>  

### 모비율의 추정  

**점추정**
확률변수 $X$: 개의 표본에서 특정 속성을 갖는 표본의 개수  
모비율 p의 점 추정량  

$\hat{p} = \dfrac{X}{n}$  
<p>&nbsp;</p>  

**구간추정**
n이 충분히 클 때 표본비율은 정규분포라 할 수 있다. (중극정)  
표본에 대해 조사를 한후 조건을 만족하는 표본비울을 $p$라고 했을 때
- n이 충분히 크다는 것은 $n\hat{p} > 5,\  n(1-\hat{p}) > 5$일 때를 의미한다.  
- 그 때 표본비율은 $X \sim N(np, np(1-p))$의 분포를 따른다. (이항분포)  

표본평균과 마찬가지로 표본비율의 신뢰구간에 따른 구간을 구할 수 있다.  
모비율 $p$의 $100(1-\alpha)%$ 신뢰구간  

$(\hat{p} - z_{\alpha / 2} \sqrt{ \dfrac{\hat{p} (1 - \hat{p})}{n}} ,\ \hat{p} + z_{\alpha / 2} \sqrt{ \dfrac{\hat{p} (1 - \hat{p})}{n}} )$  

<p>&nbsp;</p>  


## 검정 (black)  

### 가설검정
주장을 표본을 바탕으로 기준에 따라 신뢰할 수 있는지 검증하는 것이 **가설검정**  

**가설검증의 원리**  
귀무가설: 사실인지 아닌지 검증하고 싶은 가설
대립가설: 귀무가설과 대립하고있는 가설. 귀무가설이 기각되면 선택된다.  

귀무가설을 참이라고 가정할 때, 랜덤하게 선택한 표본에서 귀무가설의 결과가 나올 확률을 계산  
=> 이 확률이 기준치보다 낮다면 귀무가설을 기각하고 대립가설을 채택한다.  

확률이 낮다는 기준점 필요  
=> 유의 수준 $\alpha$ 도입  
<p>&nbsp;</p>  


**검정의 단계**  
- 귀무가설과 대립가설을 설정한다. 
- 유의수준 $\alpha$를 설정한다. (보통 0.05)  
- $\bar{X}$의 값을 표준정규확률변수(검정통계량이라고 함) $Z$ 로 변환한다.  

$\quad \quad Z = \dfrac{\bar{X} - \mu}{S / \sqrt{n}} \sim N(0, 1)$  

- $z_\alpha$보다 큰 지를 검토한다.  
  - $Z$값이 z_\alpha$보다 크다면 귀무가설을 기각한다.  
  - 작다면 귀무가설을 채택한다.  
<p>&nbsp;</p>  

### 모평균의 검정  

실제 모평균의 값이 $\mu$, 귀무가설에서 주장하는 값이 $\mu_0$ 일 때  

귀무가설은 아래와 같이 사용되고  

$H_0: \mu = \mu_0$

대립가설은 아래 3가지를 많이 사용한다.  
1. $H_1: \mu > \mu_0$  
2. $H_1: \mu < \mu_0$  
3. $H_1: \mu \neq \mu_0$  
<p>&nbsp;</p>  



**기각역**  
대립가설이 뭐냐에 따라서 달라지게 된다.  
1. $Z > z_\alpha$
2. $Z<-z_\alpha$
3. $\lvert Z \rvert > z_{\alpha/2}$

$Z = \dfrac{\bar{X} - \mu}{S / \sqrt{n}} \sim N(0, 1)$  
로 $Z$구해서 비교해보면 된다.  
<p>&nbsp;</p>  


## 교차 엔트로피 (Cross Entropy)  

### 자기정보 (self-information)  

$i(A)$로 표현되고 정보의 양을 나타내며 아래와 같이 정의된다.  

$i(A) = log_b(1/P(A)) = -log_bP(A)$  

**왜 -로그를 사용했을까?**  
처음 만든사람이 확률이 높은 사건은 정보가 적고  
확률이 낮은 사건은 정보가 많다고 생각하고 만들었다고 한다.  
그 차이는 로그함수에 비례한다고 생각해서 이렇게 만들었나보다  
<p>&nbsp;</p>  

**정보의 단위**  
정보의 양을 구할 때 사용한 로그함수의 밑에 따라 단위를 다르게 부른다.  
$b = 2$: bits  
$b = e$: nats  
$b = 10$: hartleys  
<p>&nbsp;</p>  


**특성**  

$i(AB) = log_b(\dfrac{1}{P(A)P(B)}) = log_b(\dfrac{1}{P(A)}) + log_b(\dfrac{1}{P(B)}) = i(A) + i(B)$  

독립인 두 사건이 동시에 일어났을 때의 자기정보의 양은 각각의 자기정보의 합과 같다.  
<p>&nbsp;</p>  


### 엔트로피 (entropy)  

자기정보의 평균이며 아래와 같이 표현된다.  

$H(X) = \sum_jP(A_j)i(A_j) = -\sum_jP(A_j)log_2P(A_j)$  

**특성**  

$0 \le H(X) \le log_2K$ (K:사건의 수)  

등호조건은 모든 사건의 확률이 같을 때  
<p>&nbsp;</p>  

**엔트로피의 활용**  
데이터를 표현하는데 필요한 **평균 비트수**라고 생각해도 무방하다.  
- 평균비트수를 표현
- 데이터 압축에 사용가능  
빈도에 따라서 많은 빈도를 적은 공간에 저장하는 방식으로 하면 압축이 가능하다.  
<p>&nbsp;</p>  

### 교차 엔트로피  

두 개의 확률분포가 있고 사건이 주어져 있을 때 교차 엔트로피를 정의 할 수 있다.  
두 확률분포가 얼마나 비슷한지를 수치화한 것이며 두 확률분포가 다를수록 값이 커지는 성질이 있다.  
(비슷하면 그 비슷한 값의 엔트로피 값에 가까워짐)  

아래와 같이 정의된다.  

$H(P,Q) = \sum_jP(A_j)i(A_j) = -\sum_jP(A_j)log_2Q(A_j) = -\sum_{x \in X}P(x)log_2Q(x)$

이 값은 정확한 확률분포 P를 사용했을 때의 비트수보다 크게 된다.  

$H(P,Q) = -\sum P(x)log_2Q(x) \ge -\sum P(x)log_2P(x) = H(P)$  

<p>&nbsp;</p>  
한 마디로 **$P$와 $Q$가 얼마나 비슷한지를 알 수 있다.**
<p>&nbsp;</p>  


**분류문제에서의 손실함수**  
기계학습에서는 주어진 대상이 각 그룹에 속할 확률을 제공한다.  
=> 학습을 위해서는 그 확률분포가 정답인 확률분포와 얼마나 다른지 측정필요
=> 이때! 교차 엔트로피를 사용  

**다르다는 정도를 하나의 값으로 표현해야한다.**  

- 제곱합

차이의 제곱을 모두 더해준다.  

$\sum (p_i - q_i)^2$  

학습속도가 느리며 교차 엔트로피 이전에 사용하던 방법이다.  

- 교차엔트로피

확률이 다를수록 큰 값을 가진다.  
분류문제에서는 보통 하나만 1이고 나머지는 0인 형태가 정답인데  
그 경우 정답의 엔트로피는 0 이므로 교차 엔트로피가 0이 되도록 학습시킨다.  
학습속도가 제곱합보다 더 빠르다.   
























